{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4f7be-8eec-4fae-97e5-1ad9362e2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c3cc8-22d2-4056-81fc-5ed36024061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbdf3a1-723d-4326-959c-08f80e468017",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a7ede5-a4ac-49d6-9f13-73b4f0f4e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e128c9ca-56c5-4bb4-b9b6-6bec928163a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mukesh/Library/Python/3.12/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0  How many heads of the departments are older th...   \n",
      "1  List the name, born state and age of the heads...   \n",
      "2  List the creation year, name and budget of eac...   \n",
      "3  What are the maximum and minimum budget of the...   \n",
      "4  What is the average number of employees of the...   \n",
      "\n",
      "                                             context  \\\n",
      "0                    CREATE TABLE head (age INTEGER)   \n",
      "1  CREATE TABLE head (name VARCHAR, born_state VA...   \n",
      "2  CREATE TABLE department (creation VARCHAR, nam...   \n",
      "3  CREATE TABLE department (budget_in_billions IN...   \n",
      "4  CREATE TABLE department (num_employees INTEGER...   \n",
      "\n",
      "                                              answer  \n",
      "0           SELECT COUNT(*) FROM head WHERE age > 56  \n",
      "1  SELECT name, born_state, age FROM head ORDER B...  \n",
      "2  SELECT creation, name, budget_in_billions FROM...  \n",
      "3  SELECT MAX(budget_in_billions), MIN(budget_in_...  \n",
      "4  SELECT AVG(num_employees) FROM department WHER...  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from Hugging Face\n",
    "df = pd.read_json(\"hf://datasets/b-mc2/sql-create-context/sql_create_context_v4.json\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e7e85f-445a-4265-b600-044d6f61b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for natural language prompt and SQL pairs\n",
    "class NL2SQLDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.prompts = df['question'].tolist()\n",
    "        self.sql_queries = df['answer'].tolist()\n",
    "        self.vocab = self.build_vocab()\n",
    "\n",
    "    def build_vocab(self):\n",
    "        tokens = set()\n",
    "        for prompt, sql in zip(self.prompts, self.sql_queries):\n",
    "            tokens.update(self.tokenize(prompt))\n",
    "            tokens.update(self.tokenize(sql))\n",
    "        # Add special tokens for padding and end-of-sequence (EOS)\n",
    "        tokens.update(['<PAD>', '<EOS>'])\n",
    "        return {token: i for i, token in enumerate(sorted(tokens))}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Tokenizer splitting by spaces and punctuation\n",
    "        return re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocab[token] for token in tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.encode(self.prompts[idx])\n",
    "        sql = self.encode(self.sql_queries[idx]) + [self.vocab['<EOS>']]  # Add EOS token at the end\n",
    "        return torch.tensor(prompt), torch.tensor(sql)\n",
    "\n",
    "# Custom collate function to pad sequences to the same length\n",
    "def collate_fn(batch):\n",
    "    prompts, sqls = zip(*batch)\n",
    "    \n",
    "    # Pad the sequences with the padding token\n",
    "    padded_prompts = pad_sequence(prompts, batch_first=True, padding_value=dataset.vocab['<PAD>'])\n",
    "    padded_sqls = pad_sequence(sqls, batch_first=True, padding_value=dataset.vocab['<PAD>'])\n",
    "    \n",
    "    return padded_prompts, padded_sqls\n",
    "\n",
    "# Load the dataset\n",
    "dataset = NL2SQLDataset(df)\n",
    "# Load the dataset with the custom collate function\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39523ffd-3b8f-4713-a661-df05c9a6d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.0.8_1/libexec/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model=512, nhead=8, num_layers=6):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Embed both the source and target\n",
    "        src_emb = self.embedding(src)\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "\n",
    "        # Transformer expects input of shape [sequence_length, batch_size, d_model]\n",
    "        src_emb = src_emb.transpose(0, 1)  # Transpose to [sequence_length, batch_size, d_model]\n",
    "        tgt_emb = tgt_emb.transpose(0, 1)  # Transpose to [sequence_length, batch_size, d_model]\n",
    "\n",
    "        # Pass through transformer and then final linear layer\n",
    "        output = self.transformer(src_emb, tgt_emb)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        # Output shape [sequence_length, batch_size, vocab_size] -> transpose to [batch_size, sequence_length, vocab_size]\n",
    "        return output.transpose(0, 1)\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = len(dataset.vocab)\n",
    "output_dim = len(dataset.vocab)\n",
    "model = TransformerModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097b4a9-2362-47f5-be80-bcd832990909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 6.652109950877945\n",
      "Epoch 2/20, Loss: 6.478104196464589\n",
      "Epoch 3/20, Loss: 6.466616767431315\n",
      "Epoch 4/20, Loss: 6.461856329868205\n",
      "Epoch 5/20, Loss: 6.456198759110044\n",
      "Epoch 6/20, Loss: 6.452568362124191\n",
      "Epoch 7/20, Loss: 6.44982899730291\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab['<PAD>'])  # Ignore padding in loss calculation\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataloader, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            src, tgt = batch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Target input shifted for teacher forcing\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            loss = criterion(output.reshape(-1, output_dim), tgt[:, 1:].reshape(-1))  # Shift target by 1 for the loss calculation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "# Train the model\n",
    "train(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae49ac-4a30-4b01-b96a-84a9161f6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample from the top-k tokens during generation\n",
    "def top_k_sampling(logits, k=10):\n",
    "    # Get the top-k logits and their corresponding indices\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)\n",
    "    \n",
    "    # Apply softmax to get probabilities and sample from the top-k\n",
    "    top_k_probs = F.softmax(top_k_logits, dim=-1)\n",
    "    next_token = random.choices(top_k_indices.squeeze().tolist(), top_k_probs.squeeze().tolist())[0]\n",
    "    \n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df57b5-8fd0-4aee-a82b-2d31a162cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated generate_sql function with minimum token generation length\n",
    "def generate_sql(model, prompt, max_len=100, k=10, min_len=5):\n",
    "    model.eval()\n",
    "    tokens = dataset.encode(prompt)\n",
    "    generated = []\n",
    "    src = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
    "    tgt = torch.tensor([dataset.vocab['<PAD>']]).unsqueeze(0)  # Initial target input\n",
    "\n",
    "    for i in range(max_len):\n",
    "        output = model(src, tgt)\n",
    "        next_token_logits = output[:, -1, :]  # Get logits for the last timestep\n",
    "        next_token = top_k_sampling(next_token_logits, k)  # Sample using top-k\n",
    "        \n",
    "        generated.append(next_token)\n",
    "        \n",
    "        # Prevent the model from ending the sequence too early\n",
    "        if next_token == dataset.vocab['<EOS>'] and len(generated) < min_len:\n",
    "            continue  # Ignore EOS if less than min_len tokens are generated\n",
    "        \n",
    "        # Update target sequence\n",
    "        tgt = torch.cat([tgt, torch.tensor([[next_token]])], dim=1)\n",
    "        \n",
    "        if next_token == dataset.vocab['<EOS>']:\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b7692-e4bf-427d-9c43-f45dffcca353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop for user input\n",
    "while True:\n",
    "    user_prompt = input(\"Enter your query in natural language (or 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Generate SQL query based on user input\n",
    "    sql_tokens = generate_sql(model, user_prompt)\n",
    "    sql_query = ' '.join([list(dataset.vocab.keys())[list(dataset.vocab.values()).index(token)] for token in sql_tokens])\n",
    "    \n",
    "    print(f\"Generated SQL Query: {sql_query}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
